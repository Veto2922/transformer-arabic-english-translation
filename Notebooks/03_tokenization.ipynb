{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15d267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "data_dir = Path(\"../Data/cleaned_data\")\n",
    "tokenizer_dir = Path(\"../tokenizer_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f84518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ تم تدريب التوكنيزر للعربي والإنجليزي\n"
     ]
    }
   ],
   "source": [
    "# Train Arabic tokenizer\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=str(data_dir/\"train.cleaned.ar\"),\n",
    "    model_prefix=str(tokenizer_dir/\"spm_ar_unigram\"),\n",
    "    vocab_size=32000,\n",
    "    model_type=\"unigram\",         \n",
    "    character_coverage=0.9995,    # cover Arabic characters\n",
    "    bos_id=1, eos_id=2, pad_id=0, unk_id=3\n",
    ")\n",
    "\n",
    "# Train English tokenizer\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=str(data_dir/\"train.cleaned.en\"),\n",
    "    model_prefix=str(tokenizer_dir/\"spm_en_unigram\"),\n",
    "    vocab_size=26000,             \n",
    "    model_type=\"unigram\",         \n",
    "    character_coverage=1.0,       # cover ASCII characters\n",
    "    bos_id=1, eos_id=2, pad_id=0, unk_id=3\n",
    ")\n",
    "\n",
    "print(\"✅ Tokenizers trained for Arabic and English\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b90dfd1",
   "metadata": {},
   "source": [
    "# try them model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03681dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_dir = Path(\"../tokenizer_models\")\n",
    "\n",
    "sp_ar = spm.SentencePieceProcessor(model_file=str(tokenizer_dir/\"spm_ar_unigram.model\"))\n",
    "sp_en = spm.SentencePieceProcessor(model_file=str(tokenizer_dir/\"spm_en_unigram.model\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0092fb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18024, 5558, 7280, 3558, 6, 2518]\n"
     ]
    }
   ],
   "source": [
    "x = \"الزمالك احسن نادي في مصر\"\n",
    "print(sp_ar.encode(x, out_type=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25c6622c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁الزم', 'الك', '▁احسن', '▁نادي', '▁في', '▁مصر']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_ar.encode_as_pieces(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b8e600c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 1268, 23, 621]\n"
     ]
    }
   ],
   "source": [
    "print(sp_en.encode(\"The book is nice\", out_type=int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aae57b",
   "metadata": {},
   "source": [
    "# encode all data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2df2ca60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ train encoded and saved as IDs\n",
      "✅ validation encoded and saved as IDs\n",
      "✅ test encoded and saved as IDs\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "data_dir = Path(\"../Data/cleaned_data\")\n",
    "tokenizer_dir = Path(\"../tokenizer_models\")\n",
    "output_dir = Path(\"../Data/encoded_data\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load trained tokenizers\n",
    "sp_ar = spm.SentencePieceProcessor(model_file=str(tokenizer_dir/\"spm_ar_unigram.model\"))\n",
    "sp_en = spm.SentencePieceProcessor(model_file=str(tokenizer_dir/\"spm_en_unigram.model\"))\n",
    "\n",
    "def encode_file(in_file, out_file, tokenizer):\n",
    "    with open(in_file, \"r\", encoding=\"utf-8\") as fin, open(out_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            ids = tokenizer.encode(line.strip(), out_type=int)\n",
    "            fout.write(\" \".join(map(str, ids)) + \"\\n\")\n",
    "\n",
    "# Apply encoding for train/valid/test\n",
    "splits = [\"train\", \"validation\", \"test\"]\n",
    "for split in splits:\n",
    "    # Arabic\n",
    "    encode_file(data_dir/f\"{split}.cleaned.ar\", output_dir/f\"{split}.ids.ar\", sp_ar)\n",
    "    # English\n",
    "    encode_file(data_dir/f\"{split}.cleaned.en\", output_dir/f\"{split}.ids.en\", sp_en)\n",
    "    print(f\"✅ {split} encoded and saved as IDs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
