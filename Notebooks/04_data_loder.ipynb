{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c34a316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 8298\n",
      "95th percentile: 36.0\n",
      "99th percentile: 66.0\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "with open(\"../Data/encoded_data/train.ids.ar\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        lengths.append(len(line.strip().split()))\n",
    "\n",
    "import numpy as np\n",
    "print(\"Max length:\", np.max(lengths))\n",
    "print(\"95th percentile:\", np.percentile(lengths, 95))\n",
    "print(\"99th percentile:\", np.percentile(lengths, 99))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88fa17d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 8489\n",
      "95th percentile: 40.0\n",
      "99th percentile: 69.0\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "with open(\"../Data/encoded_data/train.ids.en\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        lengths.append(len(line.strip().split()))\n",
    "\n",
    "import numpy as np\n",
    "print(\"Max length:\", np.max(lengths))\n",
    "print(\"95th percentile:\", np.percentile(lengths, 95))\n",
    "print(\"99th percentile:\", np.percentile(lengths, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89546d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataLoader ready\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "PAD_ID = 0   # نفس اللي حددته في SentencePiece\n",
    "BOS_ID = 1\n",
    "EOS_ID = 2\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_file, trg_file, max_len=50):\n",
    "        self.src_data = self.load_file(src_file, max_len)\n",
    "        self.trg_data = self.load_file(trg_file, max_len)\n",
    "        assert len(self.src_data) == len(self.trg_data), \"Source and Target not aligned!\"\n",
    "    \n",
    "    def load_file(self, path, max_len):\n",
    "        data = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                ids = [int(x) for x in line.strip().split()]\n",
    "                # إضافة BOS/EOS وقص حسب max_len\n",
    "                ids = [BOS_ID] + ids[:max_len-2] + [EOS_ID]\n",
    "                data.append(ids)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.src_data[idx]), torch.tensor(self.trg_data[idx])\n",
    "\n",
    "# collate function for padding + masks + shifting\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "\n",
    "    # احسب max length في الباتش\n",
    "    src_max_len = max([len(s) for s in src_batch])\n",
    "    trg_max_len = max([len(t) for t in trg_batch])\n",
    "\n",
    "    # padding\n",
    "    src_padded = torch.full((len(batch), src_max_len), PAD_ID)\n",
    "    trg_padded = torch.full((len(batch), trg_max_len), PAD_ID)\n",
    "\n",
    "    for i, (src, trg) in enumerate(zip(src_batch, trg_batch)):\n",
    "        src_padded[i, :len(src)] = src\n",
    "        trg_padded[i, :len(trg)] = trg\n",
    "\n",
    "    # attention masks (1 للـ tokens و 0 للـ padding)\n",
    "    src_mask = (src_padded != PAD_ID).int()\n",
    "    trg_mask = (trg_padded != PAD_ID).int()\n",
    "\n",
    "    # decoder inputs (shifted right → يبدأ بـ BOS)\n",
    "    decoder_input = trg_padded[:, :-1]\n",
    "    # decoder targets (shifted left → ينتهي بـ EOS)\n",
    "    decoder_target = trg_padded[:, 1:]\n",
    "\n",
    "    return {\n",
    "        \"src_input\": src_padded,\n",
    "        \"src_mask\": src_mask,\n",
    "        \"decoder_input\": decoder_input,\n",
    "        \"decoder_target\": decoder_target,\n",
    "        \"trg_mask\": trg_mask\n",
    "    }\n",
    "\n",
    "# train/valid DataLoader\n",
    "train_dataset = TranslationDataset(\"../Data/encoded_data/train.ids.ar\",\n",
    "                                   \"../Data/encoded_data/train.ids.en\",\n",
    "                                   max_len=80)\n",
    "\n",
    "valid_dataset = TranslationDataset(\"../Data/encoded_data/validation.ids.ar\",\n",
    "                                   \"../Data/encoded_data/validation.ids.en\",\n",
    "                                   max_len=80)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"✅ DataLoader ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b8cb937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   1,   55,   38,   16, 1738,  622,  333,    4,    2,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0])\n",
      "tensor([  55,   38,   16, 1738,  622,  333,    4,    2,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "# print(batch[\"src_input\"])      # (batch_size, src_len)\n",
    "# print(batch[\"src_mask\"])       # (batch_size, src_len)\n",
    "print(batch[\"decoder_input\"][0])  # (batch_size, trg_len-1)\n",
    "print(batch[\"decoder_target\"][0]) # (batch_size, trg_len-1)\n",
    "# print(batch[\"trg_mask\"])       # (batch_size, trg_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
